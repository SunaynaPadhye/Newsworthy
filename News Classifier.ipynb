{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c77085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd19c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the target names (categories) and some data files by following commands.\n",
    "twenty_train.target_names #prints all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24384b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])) \n",
    "#prints first line of the first data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1369496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting features from text files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c49b699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc20361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "# Training Naive Bayes (NB) classifier on training data.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77cc7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "# The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "# We will be using the 'text_clf' going forward.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0619ffe5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c2c277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance of NB Classifier\n",
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e32a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8255310124210137"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(twenty_test.target, predicted, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfc7d2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7172,   41],\n",
       "        [ 153,  166]],\n",
       "\n",
       "       [[7084,   59],\n",
       "        [ 137,  252]],\n",
       "\n",
       "       [[7081,   57],\n",
       "        [ 136,  258]],\n",
       "\n",
       "       [[6991,  149],\n",
       "        [  87,  305]],\n",
       "\n",
       "       [[7097,   50],\n",
       "        [  87,  298]],\n",
       "\n",
       "       [[7100,   37],\n",
       "        [  97,  298]],\n",
       "\n",
       "       [[7122,   20],\n",
       "        [ 119,  271]],\n",
       "\n",
       "       [[7070,   66],\n",
       "        [  32,  364]],\n",
       "\n",
       "       [[7109,   25],\n",
       "        [  27,  371]],\n",
       "\n",
       "       [[7105,   30],\n",
       "        [  40,  357]],\n",
       "\n",
       "       [[7086,   47],\n",
       "        [  12,  387]],\n",
       "\n",
       "       [[6874,  262],\n",
       "        [  13,  383]],\n",
       "\n",
       "       [[7093,   46],\n",
       "        [ 158,  235]],\n",
       "\n",
       "       [[7111,   25],\n",
       "        [ 104,  292]],\n",
       "\n",
       "       [[7072,   66],\n",
       "        [  43,  351]],\n",
       "\n",
       "       [[6633,  501],\n",
       "        [   6,  392]],\n",
       "\n",
       "       [[6979,  189],\n",
       "        [  23,  341]],\n",
       "\n",
       "       [[7130,   26],\n",
       "        [  32,  344]],\n",
       "\n",
       "       [[7216,    6],\n",
       "        [ 181,  129]],\n",
       "\n",
       "       [[7280,    1],\n",
       "        [ 216,   35]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building confusion matrix\n",
    "sklearn.metrics.multilabel_confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d7dd30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8240839086563994"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, max_iter=12, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbc8859f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8263675044574992"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding precision for SVM\n",
    "precision_score(twenty_test.target, predicted_svm, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d588f99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7129,   84],\n",
       "        [  93,  226]],\n",
       "\n",
       "       [[7079,   64],\n",
       "        [ 120,  269]],\n",
       "\n",
       "       [[7015,  123],\n",
       "        [  86,  308]],\n",
       "\n",
       "       [[7046,   94],\n",
       "        [ 130,  262]],\n",
       "\n",
       "       [[7074,   73],\n",
       "        [  66,  319]],\n",
       "\n",
       "       [[7078,   59],\n",
       "        [  92,  303]],\n",
       "\n",
       "       [[7075,   67],\n",
       "        [  40,  350]],\n",
       "\n",
       "       [[7104,   32],\n",
       "        [  42,  354]],\n",
       "\n",
       "       [[7103,   31],\n",
       "        [  13,  385]],\n",
       "\n",
       "       [[7089,   46],\n",
       "        [  39,  358]],\n",
       "\n",
       "       [[7080,   53],\n",
       "        [   3,  396]],\n",
       "\n",
       "       [[7063,   73],\n",
       "        [  15,  381]],\n",
       "\n",
       "       [[7088,   51],\n",
       "        [ 151,  242]],\n",
       "\n",
       "       [[7087,   49],\n",
       "        [  56,  340]],\n",
       "\n",
       "       [[7064,   74],\n",
       "        [  15,  379]],\n",
       "\n",
       "       [[7009,  125],\n",
       "        [  22,  376]],\n",
       "\n",
       "       [[7022,  146],\n",
       "        [  28,  336]],\n",
       "\n",
       "       [[7120,   36],\n",
       "        [  26,  350]],\n",
       "\n",
       "       [[7196,   26],\n",
       "        [ 136,  174]],\n",
       "\n",
       "       [[7262,   19],\n",
       "        [ 152,   99]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building confusion matrix for SVM\n",
    "sklearn.metrics.multilabel_confusion_matrix(twenty_test.target, predicted_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08d840e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "# Here, we are creating a list of parameters for which we would like to do performance tuning. \n",
    "# All the parameters name start with the classifier name (remember the arbitrary name we gave). \n",
    "# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d43ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we create an instance of the grid search by passing the classifier, parameters \n",
    "# and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10c873db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9157684864695698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the best mean score and the params\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "gs_clf.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9c60a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8361656930430165"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy of NB after hyperparameter tuning\n",
    "predicted_gs_nb = gs_clf.predict(twenty_test.data)\n",
    "np.mean(predicted_gs_nb == twenty_test.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3273cbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8351706629293817"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision of NB after hyperparameter tuning\n",
    "precision_score(twenty_test.target, predicted_gs_nb, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d8c81cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9051618841994754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf-svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly doing grid search for SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], \n",
    "                  'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb6b0346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8351035581518853"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy of SVM after hyperparameter tuning\n",
    "predicted_gs_svm = gs_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_gs_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4ff2af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8384991589890454"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision of SVM after hyperparameter tuning\n",
    "precision_score(twenty_test.target, predicted_gs_svm, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d34b3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK\n",
    "# Removing stop words\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ec90106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sunayna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8167817312798725"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming Code\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), \n",
    "                             ('mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted_mnb_stemmed == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24693971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8335586735879094"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#precision of NB with stemming\n",
    "precision_score(twenty_test.target, predicted_mnb_stemmed, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f12711d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8240839086563994"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy of SVM with stemming\n",
    "text_svm_stemmed = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, \n",
    "                                                   max_iter=12, random_state=42))])\n",
    "text_svm_stemmed = text_svm_stemmed.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted_svm_stemmed = text_svm_stemmed.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted_svm_stemmed == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1e76693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8263675044574992"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#precision of SVM with stemming\n",
    "precision_score(twenty_test.target, predicted_svm_stemmed, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684370d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
